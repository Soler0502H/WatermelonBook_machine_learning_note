## 一、机器学习的定义

机器学习是一门学科，致力于研究如何通过计算的手段，利用经验来玫善系统自身的性能在计算机系统中，"经验"通常以"数据"形式存在。

机器学习所研究的主要内容，是关于在计算机上从数据中产生"模型" (model) 的算法，即"学习算法" (learning algorithm). 有了学习算法，把经验数据提供给它，它就能基于这些数据产生模型;在面对新的情况时(例 如看到一个没剖开的西瓜)，模型会给我们提供相应的判断(例如好瓜) 。
 
**经验就是数据**，而机器学习就是让计算机从数据中产生出模型的算法，即学习算法。有了学习算法，我们就可以基于这些数据产生相应的模型。
## 二、机器学习的基本术语
假设收集了一批关于西瓜的数据，例如(色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂:稍蜷;敲声=沉闷)， (色泽=浅自;根蒂t硬挺;敲声=清脆)，……，每对括号内是一条记录，"="，，意思是"取值为"

	所有记录的集合：数据集（Data set）。
	
	每一条记录是关于一个事件或对象的描述:一个实例（instance）或样本（example）。
	
	反映事件或对象在某方面的表现或性质的事项："属性" (attribute) 或"特征" (feature)。
	
	属性上的取值：属性值
	
	属性张成的空间称为"属性空间" (attribute space) 、 "样本空间" (samp1e space)或"输入空间"。
	
例如把"色泽" "根蒂" "敲声"作为三个坐标轴，则它们张成一个用于描述西瓜的三维空间，每个西瓜都可在这个空间中找到自己的坐标位置.由于空间中的每个点对应一个坐标向量，因此也把一个示例称为一个"特征向量" (feature vector)。
 
一个样本的特征数为：维数（dimensionality）,当维数非常大时，也就是现在说的“维数灾难”。

	从数据中学得模型的过程称为“学习”或者是“训练”。
	训练数据（Training Data）训练的过程中使用的数据。
	训练样本（Training Sample）：训练的每一个记录。有时候训练样本也叫做训练示例或者是训练例
	训练集（Training Set）：训练样本的集合。
	假设（hypothesis）：学得模型对应了关于数据的某种潜在规律
	真相（ground-truth）：数据的潜在规律

	预测值为离散值的问题为：分类（classification）。
	预测值为连续值的问题为：回归（regression）。
涉及多个类别时，则称为“多分类（Multi-class Classification）任务”。

预测任务是希望通过对训练 集 {(X1' Y1) , (X2 ,Y2) ,..., (Xm,Ym)} 进行学习，建立一个从输入空间 X 到输出空间 y 的映射 f: X 叶 y. 对二分类任务，通常令 Y = {-1 ，十 1} 或 {O ， l}; 对多分类任务， IYI >2; 对回归任务， Y= R，R为实数集。

得到模型之后，就需要进行预测的过程，此时称为测试。

被预测的样本叫做**测试样本（Testing Sample）**。**测试样本也叫做预测示例**。

根据训练数据是否拥有标记信息，学习任务大致可划分为两大类：**监督学习和无监督学习**

 - **监督学习（supervised learning）** ：在之前提到过的分类和回归就是监督学习的代表。训练数据是有标记信息的。
 - **无监督学习（unsupervised learning）** ：聚类则是无监督学习的代表。训练数据没有标记信息。
 
学得模型适用于新样本的能力，称为**泛化（Generalization）能力**。具有强泛化能力的模型能够很好地适用于整个样本空间。

## 三、假设空间
归纳（induction）和演绎（deduction）是科学推理的两大基本手段。
 - **归纳（induction）** ：从特殊到一般的“泛化（generalization）”过程。即从具体的事实归纳出一般性规律。
 - **演绎（deduction）** ：从一般到特殊的“特化（specialization）”过程。即从基础原理推演出具体状况。

## 四、归纳偏好
机器学习算法在学习过程中对某种类型假设的偏好，称为**归纳偏好（induction bias）简称为偏好**。
任何一个有效的机器学习算法必有一个归纳偏好，否则它将被假设空间中看似在训练集上“等效”的假设迷惑，而无法产生确定的学习结果。

归纳偏好的作用：
图中的每个训练样本是图中的一个点（x,y）,要学得一个模型与训练集一致的模型，相当于找到一条穿过所有训练样本点的曲线。
 

## 五、NFL定理
**NFL定理，No Free Lunch定理**。具体含义指: 针对某一域的所有问题，所有算法的期望性能是相同的。

假设一个算法为a，而随机胡猜的算法为b，为了简单起见，假设样本空间为χχ和假设空间为HH都是离散的。令 P(h|X,a)P(h|X,a)表示算法a基于训练数据X产生假设h的概率，再令f代表我们希望的真实目标函数。a的训练集外误差，即a 在训练集之外的所有样本上的误差为 
Eote(a|X,f)=∑h∑x∈χ-XP(x)l(h(x)≠f(x))P(h|X,a)
其中l(.)是指示函数，若⋅为真则取值1，否则取值0。
（1）x代表“样本空间”，H为“假设空间”，这两者都是离散的。
（2）X为“训练数据”，故x-X的意义是“训练集外”。这是因为算法性能，是通过衡量训练集外误差得到的，即 off-training error（ote）
（3）P(h|X,L_a)表示当使用L_a算法时，基于训练集X，产生假设h的概率。

考虑二分类问题，且真实目标函数可以是任何函数χχ{0,1},函数空间为{0,1}|χ||χ|(|χ||χ|指样本空间χχ中元素个数，对所有可能的f按均匀分布对误差求和，有 
 
从以上的结果式子可以发现，总误差和学习算法无关。对于任意的两个学习算法L_a和L_b
，都有：
 


